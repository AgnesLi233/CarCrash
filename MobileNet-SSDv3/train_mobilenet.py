# -*- coding: utf-8 -*-
"""train_MobileNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CjxLEfaZT9lvMzJQzDcXjOhsTiDjRW_T
"""

import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CocoDetection
from sklearn.model_selection import train_test_split
import cv2
import numpy as np
import os
import glob

# Get the pre-trained model
config_file = 'ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt'
frozen_model = 'frozen_inference_graph.pb'
model = cv2.dnn_DetectionModel(frozen_model, config_file)

from torchvision.transforms import functional as F

def transform(image, target):
    resize = transforms.Resize((320, 320))
    image = resize(image)
    image = F.to_tensor(image)
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
    image = normalize(image)
    return image, target

import pandas as pd
import os
from PIL import Image
from torchvision.transforms import functional as F

class OpenImagesDataset(torch.utils.data.Dataset):
    def __init__(self, root, annotations_file, transforms=None):
        self.root = root
        self.transforms = transforms
        self.df = pd.read_csv(annotations_file)
        self.imgs = sorted(os.listdir(os.path.join(root, "images")))

    def __getitem__(self, idx):
        # Load images and bbox
        img_path = os.path.join(self.root, "images", self.imgs[idx])
        img = Image.open(img_path).convert("RGB")
        # Get bounding box coordinates for each mask
        num_objs = len(self.df[self.df['ImageID'] == self.imgs[idx]])
        boxes = []
        for _, row in self.df[self.df['ImageID'] == self.imgs[idx]].iterrows():
            xmin = row['XMin']
            xmax = row['XMax']
            ymin = row['YMin']
            ymax = row['YMax']
            boxes.append([xmin, ymin, xmax, ymax])
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.ones((num_objs,), dtype=torch.int64) # Assuming we have only one class

        image_id = torch.tensor([idx])
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        iscrowd = torch.zeros((num_objs,), dtype=torch.int64) # Assuming all instances are not crowd

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms is not None:
            img, target = self.transform(img, target)

        return img, target

    def __len__(self):
        return len(self.imgs)

train_set = OpenImagesDataset(root="~/data/open_images/train", annotations_file="~/data/open_images/train-annotations-bbox.csv", transform=transform)
val_set = OpenImagesDataset(root="~/data/open_images/validation", annotations_file="~/data/open_images/validation-annotations-bbox.csv", transform=transform)

train_loader = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True, num_workers=4)
val_loader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False, num_workers=4)

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model.to(device)

params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()
    for images, targets in train_loader:
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

    print(f"Epoch {epoch}: Loss: {losses.item()}")

torch.save(model.state_dict(), '/content/drive/My Drive/Accident_Detection/model.pth')