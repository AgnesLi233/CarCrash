# -*- coding: utf-8 -*-
"""milestone2_convlstm2d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qu06n0kl_EAN-nOGisq7L7OvPynB-LQe
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install tensorflow

import numpy as np
import tensorflow as tf
import torch
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import ConvLSTM2D, Dense, Flatten, TimeDistributed, Reshape, BatchNormalization, Dropout, GlobalAveragePooling2D, MaxPooling3D
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
import os

torch.cuda.get_device_name(0)

from google.colab import drive
drive.mount('/content/drive')

frame_labels = {}
with open('/content/drive/My Drive/CarCrash/videos/Crash-1500.txt', 'r') as file:
    for line in file:
        parts = line.strip().split(',')
        file_id = parts[0]

        # Find the indices for the opening and closing brackets of the list
        start_index = line.find('[')
        end_index = line.find(']', start_index)

        # Extract and convert the list substring
        if start_index != -1 and end_index != -1:
            list_str = line[start_index:end_index+1]  # Include the closing bracket
            labels_list = eval(list_str)
            frame_labels[file_id] = labels_list
        else:
            print(f"Error parsing line: {line}")

# 'frame_labels' now contains a mapping from file IDs to frame-wise labels

def open_feature_file(file_name, feature_dir):
    # Extract the file ID from the file name
    file_id = file_name.split('/')[1].split('.')[0]

    file_path = os.path.join(feature_dir, file_name)
    with np.load(file_path, allow_pickle=True) as npz_file:
        data = npz_file['data']
        data_reshaped = data.reshape(50, 20, 64, 64)

    # Get the corresponding frame labels
    frame_label = frame_labels.get(file_id, [0] * 50)

    return data_reshaped, frame_label

# Function to generate training and testing data split
def generate_train_test_split(test_size=0.5):
    # Generate file names with the respective positive or negative prefixes
    positive_files = [f"positive/{i:06d}.npz" for i in range(1, 1501)]
    negative_files = [f"negative/{i:06d}.npz" for i in range(1501, 3001)]

    # Combine positive and negative files
    all_files = positive_files + negative_files
    labels = [1] * len(positive_files) + [0] * len(negative_files)  # 1 for positive, 0 for negative
    train_files, test_files, y_train, y_test = train_test_split(all_files, labels, test_size=test_size, stratify=labels)
    return train_files, test_files, y_train, y_test

def generate_small_train_test_split(test_size=0.2):
    # Generate a smaller set of file names for testing purposes
    positive_files = [f"positive/{i:06d}.npz" for i in range(1, 601)]
    negative_files = [f"negative/{i:06d}.npz" for i in range(1501, 2101)]

    # Combine positive and negative files
    all_files = positive_files + negative_files
    labels = [1] * len(positive_files) + [0] * len(negative_files)  # 1 for positive, 0 for negative
    train_files, test_files, y_train, y_test = train_test_split(all_files, labels, test_size=test_size, stratify=labels)
    return train_files, test_files, y_train, y_test

def create_shifting_window(data, labels, context_size, future_accident_frames):
    new_data = []
    new_labels = []

    for sequence in range(data.shape[0]):  # Iterate over each video sequence
        for i in range(data.shape[1] - context_size - future_accident_frames + 1):
            context_frames = data[sequence, i:i + context_size]
            future_frames = labels[sequence, i + context_size:i + context_size + future_accident_frames]
            label = int(np.any(future_frames == 1))

            new_data.append(context_frames)
            new_labels.append(label)

    return np.array(new_data), np.array(new_labels)

feature_dir = '/content/drive/My Drive/CarCrash/vgg16_features/'
train_files, test_files, y_train, y_test = generate_small_train_test_split()

train_data = []
train_labels = []
for file_name in train_files:
    data, labels = open_feature_file(file_name, feature_dir)
    train_data.append(data)
    train_labels.append(labels)

test_data = []
test_labels = []
for file_name in test_files:
    data, labels = open_feature_file(file_name, feature_dir)
    test_data.append(data)
    test_labels.append(labels)

train_data = np.array(train_data)
test_data = np.array(test_data)
train_labels = np.array(train_labels)
test_labels = np.array(test_labels)

train_data.shape

window_size = 35  # Number of frames to consider in each sample (adjust as needed)
future_accident_frames = 15  # Looking ahead 10 frames for accidents

X_train, y_train = create_shifting_window(train_data, train_labels, window_size, future_accident_frames)
X_test, y_test = create_shifting_window(test_data, test_labels, window_size, future_accident_frames)

X_train.shape

model = Sequential()

# Adjust the input shape to match data: (30, 20, 64, 64)
model.add(TimeDistributed(Dense(64), input_shape=(35, 20, 64, 64)))
model.add(Dropout(0.5))

# ConvLSTM2D layers
model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',
                     strides=(1, 1), return_sequences=True, kernel_regularizer=l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), padding='same'))

model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3), padding='same',
                     strides=(1, 1), return_sequences=True, kernel_regularizer=l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), padding='same'))

model.add(ConvLSTM2D(filters=16, kernel_size=(3, 3), padding='same',
                     strides=(1, 1), return_sequences=False, kernel_regularizer=l2(0.01)))
model.add(BatchNormalization())

# Global Average Pooling
model.add(GlobalAveragePooling2D())

# Output layer for binary classification
model.add(Dense(units=1, activation='sigmoid'))

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00009, verbose=1)

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0004),
              loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

history = model.fit(
    X_train, y_train,
    batch_size=16,
    validation_split=0.2,
    epochs=15,
    callbacks=[reduce_lr]
)

test_loss, test_accuracy = model.evaluate(test_data, y_test)

import zipfile
import os

# Function to unzip files
def unzip_file(zip_path, extract_to):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)

# Define the paths to your zip files (adjust as necessary)
zip_path_negative = '/content/drive/MyDrive/yolo_video_features_negative.zip'
zip_path_positive = '/content/drive/MyDrive/yolo_video_features_positive.zip'

# Define extraction paths
extract_to_negative = '/content/yolo_n'
extract_to_positive = '/content/yolo_p'

# Create directories for extraction
os.makedirs(extract_to_negative, exist_ok=True)
os.makedirs(extract_to_positive, exist_ok=True)

# Extract the files
unzip_file(zip_path_negative, extract_to_negative)
unzip_file(zip_path_positive, extract_to_positive)

import numpy as np

# Function to open and read .npz files
def open_npz_file(file_path):
    try:
        with np.load(file_path, allow_pickle=True) as data:
            return data['features']
    except Exception as e:
        print(f"Error opening file: {e}")
        return None

# Load and combine the .npz files
def load_and_combine_npz(file_directory):
    combined_features = []
    for i in range(0, 50):  # Assuming 50 files numbered from 1 to 50
        file_path = os.path.join(file_directory, f'video_{i}.npz')
        features = open_npz_file(file_path)
        if features is not None:
            # Reshape if necessary
            features_reshaped = features.reshape(-1, 20, 64, 64)
            combined_features.append(features_reshaped)
        else:
            print(f"Error in loading file: {file_path}")
    return np.concatenate(combined_features, axis=0)

# Load and combine features from both directories
negative_features = load_and_combine_npz(extract_to_negative)
positive_features = load_and_combine_npz(extract_to_positive)

negative_features.shape

def process_videos(features, num_videos=50, frames_per_video=35):
    """
    Process the videos to select the first 'frames_per_video' frames from each video.

    Parameters:
    - features: Array containing video features.
    - num_videos: Number of videos.
    - frames_per_video: Number of frames to select from each video.

    Returns:
    - Processed features array.
    """
    processed_videos = []
    for i in range(num_videos):
        start_index = i * 50  # Assuming each video has 50 frames initially
        end_index = start_index + frames_per_video
        processed_videos.append(features[start_index:end_index])
    return np.array(processed_videos)

# Process negative and positive features
processed_negative = process_videos(negative_features)
processed_positive = process_videos(positive_features)

# Combine the processed features
combined_features = np.concatenate([processed_negative, processed_positive], axis=0)

combined_features.shape

# Creating labels: 0 for negative, 1 for positive
combined_labels = np.concatenate([np.zeros(processed_negative.shape[0]),
                                  np.ones(processed_positive.shape[0])])

# Evaluate the model on the combined dataset
import tensorflow as tf


evaluation = model.evaluate(combined_features, combined_labels)
print("Evaluation on combined dataset:", evaluation)